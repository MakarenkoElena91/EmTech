{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from: https://keras.io/examples/mnist_cnn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data, split between train and test sets\n",
    "(image_train, label_train), (image_test, label_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Putting data_format to \n",
    "# channel_first, you say that for every layer your tensor will have this shape: \n",
    "# (batch, channels, height, width), \n",
    "# but for channel_last you gonna have (batch, height, width, channels).\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    image_train = image_train.reshape(image_train.shape[0], 1, img_rows, img_cols)\n",
    "    image_test = image_test.reshape(image_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    image_train = image_train.reshape(image_train.shape[0], img_rows, img_cols, 1)\n",
    "    image_test = image_test.reshape(image_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "image_train = image_train.astype('float32')\n",
    "image_test = image_test.astype('float32')\n",
    "image_train /= 255\n",
    "image_test /= 255\n",
    "print('image_train shape:', image_train.shape)\n",
    "print(image_train.shape[0], 'train samples')\n",
    "print(image_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "label_train = keras.utils.to_categorical(label_train, num_classes)\n",
    "label_test = keras.utils.to_categorical(label_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Uncomment the next line to see how data looks like\n",
    "# print(image_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 276s 5ms/step - loss: 0.2642 - accuracy: 0.9184 - val_loss: 0.0626 - val_accuracy: 0.9804\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 256s 4ms/step - loss: 0.0918 - accuracy: 0.9729 - val_loss: 0.0401 - val_accuracy: 0.9858\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 280s 5ms/step - loss: 0.0681 - accuracy: 0.9807 - val_loss: 0.0338 - val_accuracy: 0.9884\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 289s 5ms/step - loss: 0.0569 - accuracy: 0.9827 - val_loss: 0.0328 - val_accuracy: 0.9885\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 293s 5ms/step - loss: 0.0483 - accuracy: 0.9858 - val_loss: 0.0281 - val_accuracy: 0.9908\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 265s 4ms/step - loss: 0.0429 - accuracy: 0.9872 - val_loss: 0.0337 - val_accuracy: 0.9888\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 240s 4ms/step - loss: 0.0374 - accuracy: 0.9891 - val_loss: 0.0291 - val_accuracy: 0.9906\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 240s 4ms/step - loss: 0.0354 - accuracy: 0.9894 - val_loss: 0.0311 - val_accuracy: 0.9899\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 240s 4ms/step - loss: 0.0327 - accuracy: 0.9897 - val_loss: 0.0260 - val_accuracy: 0.9909\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 240s 4ms/step - loss: 0.0318 - accuracy: 0.9904 - val_loss: 0.0242 - val_accuracy: 0.9915\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 272s 5ms/step - loss: 0.0294 - accuracy: 0.9909 - val_loss: 0.0267 - val_accuracy: 0.9913\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 283s 5ms/step - loss: 0.0271 - accuracy: 0.9919 - val_loss: 0.0253 - val_accuracy: 0.9909\n",
      "Test loss: 0.0252721424445961\n",
      "Test accuracy: 0.9908999800682068\n"
     ]
    }
   ],
   "source": [
    "model.fit(image_train, label_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(image_test, label_test))\n",
    "# evaluate the model\n",
    "score = model.evaluate(image_test, label_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"mnistModel.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f33f01595d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict = model.predict(image_test[16:7])\n",
    "\n",
    "plt.imshow(image_test[6].reshape(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
